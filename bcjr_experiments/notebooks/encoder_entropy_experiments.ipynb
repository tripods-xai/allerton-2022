{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.codes import ENCODERS, NonsystematicEncoders, SystematicEncoders, SystematicDecoders, NonsystematicDecoders, DECODERS\n",
    "from src.dataclasses import NonsystematicTurboEncoderSpec, SystematicTurboEncoderSpec\n",
    "from src.channels import CHANNELS, NoisyChannels\n",
    "from src.channelcoding.interleavers import FixedPermuteInterleaver\n",
    "from src.channelcoding.codes import ProjectionCode\n",
    "from src.channelcoding.bcjr import (BCJRDecoder, SystematicTurboRepeater, TurboDecoder)\n",
    "from src.channelcoding.metrics import cross_entropy_with_logits\n",
    "from src.utils import sigma2snr, snr2sigma\n",
    "from src.channelcoding.utils import enumerate_binary_inputs\n",
    "from dataclasses import asdict\n",
    "from pprint import pprint\n",
    "from typing import cast\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_estimate(log_conditional_density_matrix, indices):\n",
    "    # log_conditional_density_matrix is Received=m x Sent=m\n",
    "    # indices is 1D array of subset of [m]\n",
    "    \"\"\"\n",
    "    Suppose we have random variable Y, X, U w/ joint distribution. Suppose we sample {(y_i, x_i, u_i)| i =1 to n}. \n",
    "    Then the log conditional density matrix entry (i,j) is ln f_(Y|X = x_j)(y_i). That is, the conditional density \n",
    "    of Y given that X = x_j evaluated at y_i. If were to take exp and take expectation over all choices for X, then \n",
    "    I would get f_Y(y_i), the density of Y evaluated at y_i. Specifying the indices, allows me to get the density \n",
    "    conditioned on X being in some subset of its values specified by the choice of indicies. Usually we want to\n",
    "    restrict indices based on some relationship between X and U we want satisfied (e.g U=1)\n",
    "    \"\"\"\n",
    "    filtered_log_conditional_density_matrix = tf.gather(tf.gather(log_conditional_density_matrix, indices, axis=1), indices, axis=0)\n",
    "    num_elems = tf.cast(tf.size(indices), tf.float32)\n",
    "    log_density_estimate = tf.math.reduce_logsumexp(filtered_log_conditional_density_matrix, axis=1)\n",
    "    return tf.math.log(num_elems) - 1 / num_elems * tf.math.reduce_sum(log_density_estimate, axis=0)\n",
    "    \n",
    "def mean_conditional_entropy_estimate(log_conditional_density_matrix, inputs):\n",
    "    # inputs is 2D array of m x n inputs. m is # of samples, n is # of bits\n",
    "    # Requires that for each bit there be an input with a 1 and one with a 0.\n",
    "    m, n = inputs.shape\n",
    "    entropys = tf.TensorArray(dtype=tf.float32, size=n, clear_after_read=True)\n",
    "    for i in tf.range(n):\n",
    "        bool_1 = inputs[:, i] == 1.\n",
    "        bool_0 = ~bool_1\n",
    "        entropy_0 = entropy_estimate(log_conditional_density_matrix, tf.reshape(tf.where(bool_0), (-1)))\n",
    "        entropy_1 = entropy_estimate(log_conditional_density_matrix, tf.reshape(tf.where(bool_1), (-1)))\n",
    "        entropys = entropys.write(i, 0.5 * entropy_0 + 0.5 * entropy_1)  # Hardcoded to think inputs are uniform dist\n",
    "    return tf.reduce_mean(entropys.stack())\n",
    "\n",
    "def get_squared_distance_matrix(Y, X):\n",
    "    \"\"\"\n",
    "    both Y and X are m x n matrices where m is number of samples and n is number of features.\n",
    "    This computes the distance between the Y_i and X_j\n",
    "    \"\"\"\n",
    "    # Inner products\n",
    "    i_prods = Y @ tf.transpose(X, perm=(1, 0))  # Matrix Multiplication\n",
    "    y_norms = tf.square(tf.norm(Y, axis=1, keepdims=True))\n",
    "    x_norms_T = tf.square(tf.transpose(tf.norm(X, axis=1, keepdims=True), perm=(1, 0)))\n",
    "    return y_norms - 2 * i_prods + x_norms_T\n",
    "    \n",
    "\n",
    "def get_gaussian_log_cond_matrix(Y, X, variance):\n",
    "    squared_distance_matrix = get_squared_distance_matrix(Y, X)\n",
    "    n = tf.cast(Y.shape[1], dtype=tf.float32)\n",
    "    return (-n / 2) * tf.math.log(2 * math.pi * variance) - (1 / variance) * squared_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function src.channels.awgn(sigma, **kwargs) -> src.channelcoding.channels.AWGN>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# encoder_spec\n",
    "\n",
    "message_length = 10\n",
    "interleaver = FixedPermuteInterleaver(message_length)\n",
    "\n",
    "channel_factory = CHANNELS[NoisyChannels.AWGN]\n",
    "channel_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 10)\n",
      "Sample Size 10240\n",
      "Using snr: -1.5000000000000002\n",
      "Using sigma: 1.1885022274370185\n",
      "using nonsystematic codes\n",
      "Interleaver: [3 9 0 6 4 2 5 8 1 7]\n",
      "Encoder turboae-binary-exact\n",
      "NonsystematicTurboEncoderSpec(noninterleaved_code=<src.channelcoding.encoders.GeneralizedConvolutionalCode object at 0x7f03002474f0>, interleaved_code=<src.channelcoding.encoders.GeneralizedConvolutionalCode object at 0x7f033ec9b670>)\n",
      "Using just noninterleaved code\n",
      "Cross Entropy: 1.2406785488128662\n",
      "Constellation Entropy: 53.258453369140625\n",
      "Mean Conditional Entropy: 53.0228385925293\n",
      "Input entropy: 0.6931471824645996\n",
      "Entropy score estimate: 0.4575309753417969\n",
      "Decoder KL Divergence estimate: 0.7831475734710693\n"
     ]
    }
   ],
   "source": [
    "systematic = False\n",
    "\n",
    "repeats = 10\n",
    "all_possible_inputs = tf.cast(enumerate_binary_inputs(message_length), dtype=tf.float32)\n",
    "print(all_possible_inputs.shape)\n",
    "sample_size = repeats * all_possible_inputs.shape[0]\n",
    "# sample_size = 10000\n",
    "# messages = tf.cast(tf.random.uniform((sample_size, message_length), minval=0, maxval=2, dtype=tf.int32), tf.float32)\n",
    "messages = tf.repeat(all_possible_inputs, repeats=repeats, axis=0)\n",
    "assert messages.shape[0] == sample_size\n",
    "print(f\"Sample Size {sample_size}\")\n",
    "\n",
    "snr = -1.5\n",
    "sigma = snr2sigma(snr)\n",
    "print(f\"Using snr: {sigma2snr(sigma)}\")\n",
    "print(f\"Using sigma: {sigma}\")\n",
    "variance = sigma ** 2\n",
    "channel = channel_factory(sigma=sigma, block_len=message_length)\n",
    "\n",
    "if not systematic:\n",
    "    print(\"using nonsystematic codes\")\n",
    "    ##### Nonsystematic Codes - Start\n",
    "    # encoder_name = NonsystematicEncoders.TURBOAE_APPROXIMATED_NONSYS\n",
    "    encoder_name = NonsystematicEncoders.TURBOAE_BINARY_EXACT\n",
    "    encoder_spec = cast(NonsystematicTurboEncoderSpec, ENCODERS[encoder_name]())\n",
    "\n",
    "    print(f\"Interleaver: {interleaver.permutation}\")\n",
    "    print(f\"Encoder {encoder_name}\")\n",
    "    print(encoder_spec)\n",
    "\n",
    "    ### This is whole code with interleaver\n",
    "    # encoder = encoder_spec.noninterleaved_code \\\n",
    "    #     .concat(\n",
    "    #         interleaver.and_then(encoder_spec.interleaved_code)\n",
    "    #     )\n",
    "    \n",
    "    # non_interleaved_bcjr = BCJRDecoder(\n",
    "    #     encoder_spec.noninterleaved_code,\n",
    "    #     channel, use_max=False\n",
    "    # )\n",
    "    # interleaved_bcjr = BCJRDecoder(\n",
    "    #     encoder_spec.interleaved_code,\n",
    "    #     channel, use_max=False\n",
    "    # )\n",
    "\n",
    "    # decoder = DECODERS[NonsystematicDecoders.BASIC](\n",
    "    #     decoder1=non_interleaved_bcjr,\n",
    "    #     decoder2=interleaved_bcjr,\n",
    "    #     interleaver=interleaver,\n",
    "    #     num_iter=6\n",
    "    # )\n",
    "    \n",
    "    ### This one is just noninterleaved code\n",
    "    print(\"Using just noninterleaved code\")\n",
    "    encoder = encoder_spec.noninterleaved_code\n",
    "    \n",
    "    decoder = BCJRDecoder(\n",
    "        encoder_spec.noninterleaved_code,\n",
    "        channel, use_max=False\n",
    "    )\n",
    "    ##### Nonsystematic Codes - End\n",
    "\n",
    "if systematic:\n",
    "    print(\"Using systematic codes\")\n",
    "    ###### Systematic codes\n",
    "    encoder_name = SystematicEncoders.TURBOAE_APPROXIMATED_RSC2\n",
    "    # encoder_name = SystematicEncoders.TURBOAE_BINARY_EXACT_RSC\n",
    "    encoder_spec = cast(SystematicTurboEncoderSpec, ENCODERS[encoder_name]())\n",
    "\n",
    "    print(f\"Interleaver: {interleaver.permutation}\")\n",
    "    print(f\"Encoder {encoder_name}\")\n",
    "    print(encoder_spec)\n",
    "    interleaved_code_with_systematic = encoder_spec.interleaved_code.with_systematic()\n",
    "\n",
    "    ### This is whole code with interleaver\n",
    "    encoder = encoder_spec.systematic_code \\\n",
    "                .concat(\n",
    "                    interleaver.and_then(interleaved_code_with_systematic).and_then(ProjectionCode((1,)))\n",
    "                )\n",
    "\n",
    "    ### This is just systematic stream\n",
    "    # encoder = encoder_spec.systematic_code\n",
    "\n",
    "    non_interleaved_bcjr = BCJRDecoder(\n",
    "                # systematic_code.trellis,\n",
    "                encoder_spec.systematic_code,\n",
    "                channel, use_max=False\n",
    "            )\n",
    "    interleaved_bcjr = BCJRDecoder(\n",
    "            # interleaved_code.trellis.with_systematic(),\n",
    "            interleaved_code_with_systematic,\n",
    "            channel, use_max=False\n",
    "        )\n",
    "    repeater = SystematicTurboRepeater(\n",
    "        num_noninterleaved_streams=non_interleaved_bcjr.num_input_channels, \n",
    "        interleaver=interleaver\n",
    "    )\n",
    "    decoder = repeater.and_then(\n",
    "        DECODERS[SystematicDecoders.HAZZYS](\n",
    "            decoder1=non_interleaved_bcjr,\n",
    "            decoder2=interleaved_bcjr,\n",
    "            interleaver=interleaver,\n",
    "            num_iter=6\n",
    "        )\n",
    "    )\n",
    "\n",
    "###### Systematic Codes - End\n",
    "\n",
    "encoded_messages = encoder(messages[..., None])  # Add a channel axis\n",
    "received_messages = channel(encoded_messages)\n",
    "decoded_confidence = decoder(received_messages)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy_with_logits(messages[..., None], decoded_confidence)[\"cross_entropy\"])\n",
    "print(f\"Cross Entropy: {cross_entropy}\")\n",
    "\n",
    "flat_encoded_messages = tf.reshape(encoded_messages, (sample_size, -1))\n",
    "flat_received_messages = tf.reshape(received_messages, (sample_size, -1))\n",
    "\n",
    "\n",
    "gaussian_log_cond_matrix = get_gaussian_log_cond_matrix(flat_received_messages, flat_encoded_messages, variance=variance)\n",
    "constellation_entropy = entropy_estimate(gaussian_log_cond_matrix, indices=tf.range(sample_size))\n",
    "conditional_entropy = mean_conditional_entropy_estimate(gaussian_log_cond_matrix, messages)\n",
    "input_entropy = tf.math.log(2.)\n",
    "# input_entropy_estimate = \n",
    "\n",
    "print(f\"Constellation Entropy: {constellation_entropy}\")\n",
    "print(f\"Mean Conditional Entropy: {conditional_entropy}\")\n",
    "print(f\"Input entropy: {input_entropy}\")\n",
    "\n",
    "entropy_score_estimate = conditional_entropy + input_entropy - constellation_entropy\n",
    "print(f\"Entropy score estimate: {entropy_score_estimate}\")\n",
    "\n",
    "kl_divergence_estimate = cross_entropy - entropy_score_estimate\n",
    "print(f\"Decoder KL Divergence estimate: {kl_divergence_estimate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snr: -1.5000000000000002\n",
      "Using sigma: 1.1885022274370185\n",
      "Using systematic codes\n",
      "Interleaver: [ 6 15  2  4 16 17 14 19  7 11 18 13  3  8  5 10  1  0 12  9]\n",
      "Encoder turbo-155-7\n",
      "SystematicTurboEncoderSpec(systematic_code=<src.channelcoding.encoders.TrellisCode object at 0x7f039930d040>, interleaved_code=<src.channelcoding.encoders.GeneralizedConvolutionalCode object at 0x7f02b83d4220>)\n",
      "Cross Entropy: 0.22850501537322998\n",
      "Constellation Entropy: 155.32859802246094\n",
      "Mean Conditional Entropy: 154.68893432617188\n",
      "Input entropy: 0.6931471824645996\n",
      "Entropy score estimate: 0.0534820556640625\n",
      "Decoder KL Divergence estimate: 0.17502295970916748\n"
     ]
    }
   ],
   "source": [
    "systematic = True\n",
    "\n",
    "sample_size = 10000\n",
    "messages = tf.cast(tf.random.uniform((sample_size, message_length), minval=0, maxval=2, dtype=tf.int32), tf.float32)\n",
    "\n",
    "snr = -1.5\n",
    "sigma = snr2sigma(snr)\n",
    "print(f\"Using snr: {sigma2snr(sigma)}\")\n",
    "print(f\"Using sigma: {sigma}\")\n",
    "variance = sigma ** 2\n",
    "channel = channel_factory(sigma=sigma, block_len=message_length)\n",
    "\n",
    "if not systematic:\n",
    "    print(\"using nonsystematic codes\")\n",
    "    ##### Nonsystematic Codes - Start\n",
    "    encoder_name = NonsystematicEncoders.TURBOAE_APPROXIMATED_NONSYS\n",
    "    # encoder_name = NonsystematicEncoders.TURBOAE_BINARY_EXACT\n",
    "    encoder_spec = cast(NonsystematicTurboEncoderSpec, ENCODERS[encoder_name]())\n",
    "\n",
    "    print(f\"Interleaver: {interleaver.permutation}\")\n",
    "    print(f\"Encoder {encoder_name}\")\n",
    "    print(encoder_spec)\n",
    "\n",
    "    ### This is whole code with interleaver\n",
    "    encoder = encoder_spec.noninterleaved_code \\\n",
    "        .concat(\n",
    "            interleaver.and_then(encoder_spec.interleaved_code)\n",
    "        )\n",
    "\n",
    "    ### This one is just noninterleaved code\n",
    "    # encoder = encoder_spec.noninterleaved_code\n",
    "    \n",
    "    non_interleaved_bcjr = BCJRDecoder(\n",
    "        encoder_spec.noninterleaved_code,\n",
    "        channel, use_max=False\n",
    "    )\n",
    "    interleaved_bcjr = BCJRDecoder(\n",
    "        encoder_spec.interleaved_code,\n",
    "        channel, use_max=False\n",
    "    )\n",
    "\n",
    "    decoder = DECODERS[NonsystematicDecoders.BASIC](\n",
    "        decoder1=non_interleaved_bcjr,\n",
    "        decoder2=interleaved_bcjr,\n",
    "        interleaver=interleaver,\n",
    "        num_iter=6\n",
    "    )\n",
    "    ##### Nonsystematic Codes - End\n",
    "\n",
    "if systematic:\n",
    "    print(\"Using systematic codes\")\n",
    "    ###### Systematic codes\n",
    "    # encoder_name = SystematicEncoders.TURBOAE_APPROXIMATED_RSC2\n",
    "    # encoder_name = SystematicEncoders.TURBOAE_BINARY_EXACT_RSC\n",
    "    encoder_name = SystematicEncoders.TURBO_155_7\n",
    "    encoder_spec = cast(SystematicTurboEncoderSpec, ENCODERS[encoder_name]())\n",
    "\n",
    "    print(f\"Interleaver: {interleaver.permutation}\")\n",
    "    print(f\"Encoder {encoder_name}\")\n",
    "    print(encoder_spec)\n",
    "    interleaved_code_with_systematic = encoder_spec.interleaved_code.with_systematic()\n",
    "\n",
    "    ### This is whole code with interleaver\n",
    "    encoder = encoder_spec.systematic_code \\\n",
    "                .concat(\n",
    "                    interleaver.and_then(interleaved_code_with_systematic).and_then(ProjectionCode((1,)))\n",
    "                )\n",
    "\n",
    "    ### This is just systematic stream\n",
    "    # encoder = encoder_spec.systematic_code\n",
    "\n",
    "    non_interleaved_bcjr = BCJRDecoder(\n",
    "                # systematic_code.trellis,\n",
    "                encoder_spec.systematic_code,\n",
    "                channel, use_max=False\n",
    "            )\n",
    "    interleaved_bcjr = BCJRDecoder(\n",
    "            # interleaved_code.trellis.with_systematic(),\n",
    "            interleaved_code_with_systematic,\n",
    "            channel, use_max=False\n",
    "        )\n",
    "    repeater = SystematicTurboRepeater(\n",
    "        num_noninterleaved_streams=non_interleaved_bcjr.num_input_channels, \n",
    "        interleaver=interleaver\n",
    "    )\n",
    "    decoder = repeater.and_then(\n",
    "        DECODERS[SystematicDecoders.HAZZYS](\n",
    "            decoder1=non_interleaved_bcjr,\n",
    "            decoder2=interleaved_bcjr,\n",
    "            interleaver=interleaver,\n",
    "            num_iter=6\n",
    "        )\n",
    "    )\n",
    "\n",
    "###### Systematic Codes - End\n",
    "\n",
    "encoded_messages = encoder(messages[..., None])  # Add a channel axis\n",
    "received_messages = channel(encoded_messages)\n",
    "decoded_confidence = decoder(received_messages)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy_with_logits(messages[..., None], decoded_confidence)[\"cross_entropy\"])\n",
    "print(f\"Cross Entropy: {cross_entropy}\")\n",
    "\n",
    "flat_encoded_messages = tf.reshape(encoded_messages, (sample_size, -1))\n",
    "flat_received_messages = tf.reshape(received_messages, (sample_size, -1))\n",
    "\n",
    "\n",
    "gaussian_log_cond_matrix = get_gaussian_log_cond_matrix(flat_received_messages, flat_encoded_messages, variance=variance)\n",
    "constellation_entropy = entropy_estimate(gaussian_log_cond_matrix, indices=tf.range(sample_size))\n",
    "conditional_entropy = mean_conditional_entropy_estimate(gaussian_log_cond_matrix, messages)\n",
    "input_entropy = tf.math.log(2.)\n",
    "# input_entropy_estimate = \n",
    "\n",
    "print(f\"Constellation Entropy: {constellation_entropy}\")\n",
    "print(f\"Mean Conditional Entropy: {conditional_entropy}\")\n",
    "print(f\"Input entropy: {input_entropy}\")\n",
    "\n",
    "entropy_score_estimate = conditional_entropy + input_entropy - constellation_entropy\n",
    "print(f\"Entropy score estimate: {entropy_score_estimate}\")\n",
    "\n",
    "kl_divergence_estimate = cross_entropy - entropy_score_estimate\n",
    "print(f\"Decoder KL Divergence estimate: {kl_divergence_estimate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1\n",
    "Let $X, Y$ be [[Random Variable]]s so that\n",
    "\n",
    "$$\\begin{align*}\n",
    "&X \\sim \\text{Unif}\\{\\pm 1\\}\\\\\n",
    "&Y \\sim  X \\cdot \\text{Unif}[0, 1]\n",
    "\\end{align*}$$\n",
    "Then\n",
    "$$\\begin{align*}\n",
    "f_{Y}(y) &= \\frac{1}{2} \\mathbb{1}_{[0, 1]}(y) + \\frac{1}{2} \\mathbb{1}_{[-1,0]}(y)\\\\\n",
    "&=\\frac{1}{2}\\mathbb{1}_{[-1,1]}(y),\n",
    "\\end{align*}$$\n",
    "that is, $Y \\sim \\text{Unif}[-1, 1]$. By [[Differential Entropy of Continuous Uniform Random Variable]] we have that\n",
    "$$\\mathbb{H}(Y) = \\ln (1 + 1) = \\ln 2$$\n",
    "By sampling we should be able to get a similar value. In particular, suppose we sample $x_{i} \\in \\{-1, 1\\}$ and $y_{i} \\in [-1, 1]$. Then we have that\n",
    "$$\\begin{align*}\n",
    "&f_{Y|X=1} = \\mathbb{1}_{[0,1]}\\\\\n",
    "&f_{Y|X=-1} = \\mathbb{1}_{[-1,0]}\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1., -1., -1., -1.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.25897527, -0.9543897 , -0.36819065, -0.5703981 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       " array([[1., 0., 0., 0.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unif_conditional_density_matrix(m):\n",
    "    X = tf.cast(2 * tf.random.uniform((m,), minval=0, maxval=2, dtype=tf.int32) - 1, tf.float32)\n",
    "    Y = X * tf.random.uniform((m,), minval=0, maxval=1, dtype=tf.float32)\n",
    "    outer_prod = Y[:, None] * X[None, :]\n",
    "    cond_density = tf.cast(tf.logical_and(outer_prod >= 0, outer_prod <= 1), tf.float32)\n",
    "    return X, Y, cond_density\n",
    "\n",
    "unif_conditional_density_matrix(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate: 0.689943790435791\n",
      "Truth: 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "sample_size = 100\n",
    "Xvar, Yvar, fYX = unif_conditional_density_matrix(sample_size)\n",
    "logfYX = tf.math.log(fYX)\n",
    "estimate = entropy_estimate(logfYX, tf.range(sample_size))\n",
    "truth = tf.math.log(2.)\n",
    "\n",
    "print(f'Estimate: {estimate}')\n",
    "print(f'Truth: {truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 1.  1. -1. -1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int64)\n",
      "Estimate: 0.0\n",
      "Truth: 0.0\n"
     ]
    }
   ],
   "source": [
    "sample_size = 4\n",
    "Xvar, Yvar, fYX = unif_conditional_density_matrix(sample_size)\n",
    "indices_where_1 = tf.reshape(tf.where(Xvar == 1.), (-1,))\n",
    "print(Xvar)\n",
    "print(indices_where_1)\n",
    "logfYX = tf.math.log(fYX)\n",
    "estimate = entropy_estimate(logfYX, indices_where_1)\n",
    "truth = tf.math.log(1.)\n",
    "\n",
    "print(f'Estimate: {estimate}')\n",
    "print(f'Truth: {truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.  2.  3.]\n",
      " [-3. -2. -1.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 5. -3.  0.]\n",
      " [ 4. -1.  2.]], shape=(2, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[49.999996, 66.      ],\n",
       "       [19.      , 59.      ]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = tf.constant([\n",
    "    [1., 2., 3.],\n",
    "    [-3., -2. ,-1.]\n",
    "])\n",
    "X = tf.constant([\n",
    "    [5., 4.],\n",
    "    [-3., -1.],\n",
    "    [0., 2.]\n",
    "])\n",
    "X = tf.transpose(X)\n",
    "\n",
    "print(Y)\n",
    "print(X)\n",
    "\n",
    "# tf.tensordot(Y, X, axes=1)\n",
    "# Y @ X\n",
    "# tf.norm(Y, axis=1, keepdims=True)\n",
    "get_squared_distance_matrix(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 + 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a90e52e46aab268b5a60d5d4a973112b884470e0dd1e30ef308d0891c9e7699"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('turbo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
